\documentclass[10pt]{article}

%% Various useful packages and commands from different sources

\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite, url,color} % Citation numbers being automatically sorted and properly "compressed/ranged".
%\usepackage{pgfplots}
\usepackage{graphics,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{bm}
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
 \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally does.

% Compact lists
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyvrb}

% Tikz
\usepackage{tikz}
\usetikzlibrary{automata,positioning,chains,shapes,arrows}
\usepackage{pgfplots}
\usetikzlibrary{plotmarks}
\newlength\fheight
\newlength\fwidth
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\usepackage{listings} % for Matlab code
\definecolor{commenti}{rgb}{0.13,0.55,0.13}
\definecolor{stringhe}{rgb}{0.63,0.125,0.94}
\lstloadlanguages{Matlab}
\lstset{% general command to set parameter(s)
framexleftmargin=0mm,
frame=single,
keywordstyle = \color{blue},% blue keywords
identifierstyle =, % nothing happens
commentstyle = \color{commenti}, % comments
stringstyle = \ttfamily \color{stringhe}, % typewriter type for strings
showstringspaces = false, % no special string spaces
emph = {for, if, then, else, end},
emphstyle = \color{blue},
firstnumber = 1,
numbers =right, %  show number_line
numberstyle = \tiny, % style of number_line
stepnumber = 5, % one number_line after stepnumber
numbersep = 5pt,
language = {Matlab},
extendedchars = true,
breaklines = true,
breakautoindent = true,
breakindent = 30pt,
basicstyle=\footnotesize\ttfamily
}

\usepackage{array}
% http://www.ctan.org/tex-archive/macros/latex/required/tools/
\usepackage{mdwmath}
\usepackage{mdwtab}
%mdwtab.sty	-- A complete ground-up rewrite of LaTeX's `tabular' and  `array' environments.  Has lots of advantages over
%		   the standard version, and over the version in `array.sty'.
% *** SUBFIGURE PACKAGES ***
% \usepackage[tight,footnotesize]{subfigure}
\usepackage{subfig}
\usepackage[top=2.2cm, bottom=2.2cm, right=1.7cm,left=1.7cm]{geometry}
\usepackage{indentfirst}


%\setlength\parindent{0pt}
\linespread{1}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\M} {\mathtt{M}}
\newcommand{\dB} {\mathrm{dB}}
\newcommand{\tr} {\mathrm{tr}}
\newcommand{\lmod}[1] {_{\,\mathrm{mod}\,#1}}
\newcommand{\outf}[1] {\mathcal{O}(#1)}
\newcommand{\SU}[1] {\mathcal{S}(#1)}
\newcommand{\s} {\mathbf{s}}
\newcommand{\y} {\mathbf{y}}


\graphicspath{ {figures/} }
\setcounter{MaxMatrixCols}{20}

% equations are numbered section by section
%\numberwithin{equation}{section}


\begin{document}
\title{Channel Coding 15/16 - Homework 1}
\author{Michele Polese}

\maketitle

% For tikz
% Definition of blocks:
\tikzstyle{block} = [draw, rectangle, 
    minimum height=3em, minimum width=3em]
\tikzstyle{circlenode} = [draw, circle, minimum height=3em, minimum width=3em]
\tikzstyle{trellisnode} = [draw, circle, minimum height=2em, minimum width=2em]
\tikzstyle{sum} = [draw, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% PROBLEM 1 %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 1}

\subsection{Analysis of (5,1,7) binary polynomial convolutional code}

In the first exercise I will provide an analysis of the performances of the binary convolutional code defined by the triplet 
\begin{equation}
	\mathbf{g} = \mathbf{g}_3 = \begin{bmatrix}
			1 + D^2 \\
			D^2	\\
			1 + D + D^2 \\
			\end{bmatrix}
\end{equation}
This is the polynomial code $(5,1,7)$ in octal notation, whose realization diagram is in Fig.~\ref{fig:diag}. The rate is $R = 1/3$. Its memory is $\nu = 2$ and since it is a binary code with $q = 2$ the state space has size $q^\nu = 4$. %Note that every sum from now on will be a sum in $\mathbb{F}_2$ (i.e. modulo 2 sum) unless specified.

\begin{figure}[t]
\centering

% picture of realization diagram
\begin{tikzpicture}[auto, thick, node distance=3cm, >=latex']
	% Drawing the blocks of first filter :
	%node at (0,0)[right=-3mm]{\Large \textopenbullet};
	\node [input, name=input1] {};
    \node [block, name=ret1, right of=input1] (ret1) {$D$};
    \node [block, name=ret2, right of=ret1] (ret2) {$D$};
    \node [right of=ret2] (output) {$y_{2,l}$};

    % Joining blocks. 
    % Commands \draw with options like [->] must be written individually
	\draw[draw,->](input1) -- node[name=u] {$u_l$} (ret1);
	\node [sum, below of=u, yshift=-1cm] (sum1) {+};
 	\draw[->](ret1) -- node[name=s1] {$u_{l-1} = s_{1,l}$} (ret2);
 	\node [sum, below of=s1, yshift=-1cm] (sum2) {+};
 	\draw[->] (ret2) edge node[name=s2] {$u_{l-2} = s_{2,l}$} (output);
	\node [sum, below of=s2, yshift=-1cm] (sum3) {+};
	\node [sum, above of=s2, yshift=0.5cm] (sum4) {+};
 	\draw[->] (u) edge (sum1);
 	\draw[->] (s1) edge (sum2);
 	\draw[->] (s2) edge (sum3);
 	\draw[->] (s2) edge (sum4);
 	\draw[->] (u) |- (sum4);
 	\draw[->] (sum1) edge (sum2);
 	\draw[->] (sum2) edge (sum3);
 	\node[above of=output, yshift=-1.25cm] (y1) {$y_{1,l}$};
 	\node[below of=output, yshift=1.25cm] (y3) {$y_{3,l}$};
 	\draw[->] (sum4) edge (y1);
	\draw[->] (sum3) edge (y3);

\end{tikzpicture}
\caption{Realization diagram of convolution binary code with generator $\mathbf{g}$}
\label{fig:diag}
\end{figure}

% state update & output function
Let $\s_l = [s_{1, l}, s_{2, l}]^T$ be the state vector at time $l$. It can be immediately seen that the state update function $\SU{\s_l, u_l}$ is
\begin{equation}
	\s_{l+1} = \SU{\s_l, u_l} = \begin{bmatrix} 1 \\ 0 \\ \end{bmatrix} u_l + 
				\begin{bmatrix} 0 & 0 \\ 1 & 0 \\ \end{bmatrix} \s_l
\end{equation}
Let $\mathbf{y}_l = [y_{1, l}, y_{2, l}, y_{3, l}]^T$ be the codeword obtained from input $u_l$ and state $\mathbf{s}_l$. Then the output function $\outf{\s_l, u_l}$ is
\begin{equation}
	\y_l = \outf{\s_l, u_l} = \begin{bmatrix} 1 \\ 0 \\ 1 \\ \end{bmatrix} u_l + 
		\begin{bmatrix} 0 & 1 \\ 0 & 1 \\ 1 & 1 \\ \end{bmatrix} \s_l
\end{equation}

% N(a)

By using the state update function $\SU{\s_l, u_l}$ it is possible to derive the \emph{neighbors set} of each state $\mathbf{a}$, which is the set of couples $(\s_l, u_l) = (\mathbf{b}, u)$ that leads to $\mathbf{a}$:
\begin{equation}
	\mathcal{N}(\mathbf{a}) = \left\{ (\mathbf{b}, u) \middle| \mathbf{a} = \SU{\mathbf{b}, u} \right\} 
\end{equation}
Then
\begin{equation}
	\mathcal{N}(00) = \left\{ ([00], 0), ([01], 0) \right\} 
\end{equation}
\begin{equation*}
	\mathcal{N}(01) = \left\{ ([10], 0), ([11], 0) \right\} 
\end{equation*}
\begin{equation*}
	\mathcal{N}(10) = \left\{ ([00], 1), ([01], 1) \right\} 
\end{equation*}
\begin{equation*}
	\mathcal{N}(01) = \left\{ ([10], 1), ([11], 1) \right\} 
\end{equation*}

% state trans diagr
The state transition diagram is a way to represent the code in terms of possible transitions from a certain state and associated output, given a certain input. The state transition diagram of $\mathbf{g}$ can be seen in Fig.~\ref{fig:state}.

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick, node distance=4.5cm, ->,>=latex',shorten >=1pt]

	\node [circlenode] (00) {$00$};
	\node [circlenode, above right of=00] (01) {$01$};
	\node [circlenode, below right of=00] (10) {$10$};
	\node [circlenode, above right of=10] (11) {$11$};

	\path[every node]
		(00) edge [loop left] node[left] {$0, [000]$} (00)
			 edge [bend right] node[below left] {$1, [101]$} (10)
		(01) edge [bend right] node[above left] {$0, [111]$} (00)
			 edge [bend right] node[left] {$1, [010]$} (10)
		(10) edge [bend right] node[right] {$0, [001]$} (01)
			 edge [bend right] node[below right] {$1, [100]$} (11)
		(11) edge [loop right] node[right] {$1, [011]$} (11)
			 edge [bend right] node[above right] {$0, [110]$} (01);
	
\end{tikzpicture}
\caption{State transition diagram}
\label{fig:state}
\end{figure}

\subsection{Performance evaluation through theoretical bounds}

From a slightly modified version of the state transition diagram (in Fig.~\ref{fig:state_t}) it is possible to derive the expression of the transfer function that characterizes the code. The loop on $00$ is removed and this state is split into starting state $\mathbf{0}_s$ and ending state $\mathbf{0}_e$.

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick, node distance=5 cm, ->,>=latex',shorten >=1pt]
	\node [circlenode] (0s) {$\mathbf{0}_s$};
	\node [circlenode, right of=0s] (10) {$10$};
	\node [circlenode, below right of=10] (11) {$11$};
	\node [circlenode, above right of=11] (01) {$01$};
	\node [circlenode, right of=01] (0e) {$\mathbf{0}_e$};

	\path[every node]
		(0s) edge node[above] {$1, [101]$} node[below] {$x^2wz$} (10)
		(10) edge [bend left] node[above] {$0, [001]$} node[below] {$xz$} (01)
			 edge [bend right] node[above right] {$1, [100]$} node[below left] {$xwz$} (11)
		(01) edge [bend left] node[above] {$1, [010]$} node[below] {$xwz$} (10)
			 edge node[above] {$0, [111]$} node[below] {$x^3z$} (0e)
		(11) edge [loop below] node[below left] {$1, [011]$} node[below right] {$x^2wz$} (11)
			 edge [bend right] node[above left] {$0, [110]$} node[below right] {$x^2z$} (01);
\end{tikzpicture}
\caption{Modified transition diagram}
\label{fig:state_t}
\end{figure}

On each transition let $x$ be a variable whose exponent is associated with the Hamming weight of the output (i.e. if $\|\outf{\s_l, u_l}\|_H = d$, then on that edge there will be $x^d$), $w$ be a variable that represents the input weight (thus either with exponent $i$ 0 or 1) and $z$ be associated with the input length $l$ (in this case $l=1$). Then it is possible to label each transition with a $x^dw^iz^l$ triplet and define the Weight Enumerating Function (WEF):
\begin{equation}
	W_{\mathbf{s}}(x, w, z) = \sum_{d, i, l = 0}^{\infty}W_{d, i, l} x^dw^iz^l
\end{equation}
with the weight $W_{d, i, l}$ given by the number of codeword $\mathbf{y}$ starting from state $00$, ending in state $\mathbf{s}$, with Hamming norm $d$, input weight $i$ and length $l$. In particular there is linear relation between WEFs of neighbors, i.e.
\begin{equation}
	W_{\mathbf{a}}(x, w, z) = \sum_{(\mathbf{b}, u) \in \mathcal{N}(\mathbf{a})
	} W_{\mathbf{b}}(x,w,z) x^{\|\outf{\mathbf{b}, u}\|_H} w^{\|u\|_H} z
\end{equation}

Let $T(x, w, z) = W_{\mathbf{0}_e} (x, w, z)$ be the transfer function of system, i.e. the function whose terms identify the number of codewords with $d < 2d_{min}$ with a certain input weight $i$ and length $l$. This can be derived by solving the following linear system
\begin{equation}\label{eq:s1}
\begin{cases}
	W_{\mathbf{0}_s} = 1 \\ %TODO definition \\
	W_{01} = W_{11}x^2z + W_{10}xz \\
	W_{10} = x^2wz + W_{01}xwz \\
	W_{11} = W_{11}x^2wz + W_{10}xwz \\
	T = W_{\mathbf{0}_e} = W_{01}x^3z\\
\end{cases}
\end{equation}

\begin{equation}
\begin{cases}
	W_{\mathbf{0}_s} = 1 \\ %TODO definition \\
	W_{01} = W_{10}\frac{x^3wz^2}{1-x^2wz} + W_{10}xz \\
	W_{10} = x^2wz + W_{01}xwz \\
	W_{11} = W_{10}\frac{xwz}{1-x^2wz} \\
	T = W_{\mathbf{0}_e} = W_{01}x^3z\\
\end{cases}
\end{equation}
and then by focusing on $W_{01}$
\begin{equation*}
	W_{01} =  \frac{xz}{1-x^2wz}W_{10}
\end{equation*}
\begin{equation*}
	W_{01}  =  \frac{x^2wz^2}{1-x^2wz}(x + W_{01})
\end{equation*}
\begin{equation*}
	W_{01}\left(\frac{1 - x^2wz - x^2wz^2}{1-x^2wz} \right) = \frac{x^3wz^2}{1-x^2wz}
\end{equation*}
\begin{equation*}
	W_{01} = \frac{x^3wz^2}{1-x^2wz(1+z)}
\end{equation*}
and finally from~\eqref{eq:s1}
\begin{equation}\label{eq:T}
	T(x, w, z) = W_{\mathbf{0}_e}(x, w, z) = \frac{x^6wz^3}{1-x^2wz(1+z)} = x^6wz^3\sum_{k=0}^{\infty} [x^2wz(1+z)]^k
\end{equation}
% Pbit
It is possible to expand $T(x,w,z) = \sum_{d, i, l = 0}^{\infty} t_{d, i, l}x^dw^iz^l$ in order to get the coefficients $t_{d,i,l}$ that represent the number of codeword with Hamming weight $d$, associated with an input of weight $i$ and length $l$. From~\eqref{eq:T} I get
\begin{equation}\label{Texp}
	T(x, w, z) = x^6wz^3 + x^8w^2z^4 + x^8w^2z^5 + x^{10}w^3z^5 + 2x^{10}w^3z^6 + x^{10}w^3z^7 + x^6wz^3\sum_{k=4}^{\infty}[x^2wz(1+z)]^k
\end{equation}
It can be seen that $d_{min}=6$ for this code. Since the transfer function $T$ considers only codewords that do not visit more than twice $00$ in their path (i.e. it is only the starting and ending state) it provides meaningful $t_{d, i, l}$ coefficients only for $d < 2d_{min} = 12$. The last infinite sum in~\eqref{Texp} yields terms with $d \ge 2d_{min}$ that cannot be considered in the derivation of $P_{bit}$ expression. The valid $t_{d, i, l}$ coefficients are
\begin{equation*}
	t_{6,1,3} = 1
\end{equation*}
\begin{equation}\label{eq:coeff}
	t_{8,2,4} = t_{8,2,5} = 1
\end{equation}
\begin{equation*}
	t_{10, 3, 5} = t_{10, 3, 7} = 1, \quad  t_{10,3,6} = 2
\end{equation*}
From \cite{erseghe} the bound for $P_{bit}$ is
\begin{equation}
	P_{bit} \le \sum_{d=d_{min}}^n K(d)Q\left(\sqrt{2\frac{E_b}{No}Rd}\right)
\end{equation}
with $K(d) = \frac{1}{k} \sum_{\mathbf{a}\in\mathcal{U}(d)} \|\mathbf{a}\|_H$ and $\mathcal{U}_d = \left\{ \mathbf{u} \in \mathcal{U} \; \middle| \; \|\mathbf{Gu} \|_{H} = d \right\}$. For a convolutional code 
\begin{equation}
	K(d) = \frac{1}{\mu}\sum_{l=0}^{\infty} (\mu + \nu - l) \sum_{i=0}^{\infty} i t_{d,i,l} \quad d < 2d_{min}
\end{equation}
that can be upper bounded by
\begin{equation}
	K(d) \lesssim \sum_{i=0}^{\infty} i \left( \sum_{l=0}^{\infty}t_{d,i,l}\right) \quad d < 2d_{min}
\end{equation}
Then from~\eqref{eq:coeff} 
\begin{equation*}
	K(6) \lesssim 1\cdot t_{6,1,3} = 1
\end{equation*}
\begin{equation}
	K(8) \lesssim 2\cdot (t_{8,2,4} + t_{8,2,5}) = 4
\end{equation}
\begin{equation*}
	K(10) \lesssim 3\cdot (t_{10, 3, 5} + t_{10, 3, 7} + t_{10,3,6}) = 12
\end{equation*}
and eventually the expression for the bit error rate is
\begin{equation}\label{eq:BER_bound}
	P_{bit} \lesssim Q\left(\sqrt{4\frac{E_b}{N_0}} \right) + 4\cdot Q\left(\sqrt{\frac{16E_b}{3N_0}} \right) + 12 \cdot Q\left(\sqrt{\frac{20E_b}{3N_0}} \right)
\end{equation}
given that the rate is $R=1/3$. An approximate expression (but underestimated for low SNR) is
\begin{equation}\label{eq:BER_approx}
	P_{bit} \cong Q\left(\sqrt{4\frac{E_b}{N_0}} \right)
\end{equation}

% plot expected
In Fig.~\ref{fig:BER_theory} there is the plot of expressions~\eqref{eq:BER_bound} and~\eqref{eq:BER_approx}. The nominal coding gain is by definition $\gamma_c = Rd_{min} = 3.01$ dB, while the effective coding gain is the difference in dB between the value of $E_b/N_0$ for which the curves~\eqref{eq:BER_bound} and $Q\left(\sqrt{2\frac{E_b}{N_0}}\right)$ (uncoded $P_{bit}$) reach $P_{bit} = P_{target} = 10^{-5}$. It can be numerically evaluated with MATLAB and for the code under analysis is $\gamma_e = 2.935$ dB. Therefore the difference between nominal and effective coding gain is $\gamma_c - \gamma_e = 0.075$ dB.

\subsection{Performance evaluation through simulation}

% plot comparison with simulation
In order to evaluate through simulation the performance of the convolutional code with generator polynomial $\mathbf{g}$ an encoder and a Viterbi decoder were implemented in C and called from a MATLAB script using MEX functions. 
The channel implementation instead is left to a sum between vectors in MATLAB, so that the \texttt{randn} function of MATLAB can be used. Indeed the algorithm with which MATLAB generates Gaussian random noise is very efficient \cite{moler}. The standard Viterbi was implemented with backtracking in order to retrieve the symbols associated with each transition on the trellis. Moreover the neighbor set and the possible outputs were hard-coded in lookup tables. 

The performances were tested for different values of $E_b/N_0 \in [-3, 8]$, with $K$ iterations. For each iteration let $\mathbf{u}$ be the input vector of size $\mu + \nu$, $\mathbf{y}$ the encoded vector, $\mathbf{s} = \mathcal{L}(\mathbf{y}) = 2\mathbf{y} - 1$ the vector of symbols which are sent through the channel and $\mathbf{w}$ a vector of IID samples $w_i \sim N(0,1), i = 0 \dots (\mu + \nu)/R - 1$. Then the received vector $\mathbf{r}_{E_b/N_0}$ for each value of $E_b/N_0$ is 
\begin{equation}
	\mathbf{r}_{E_b/N_0} = \mathbf{s} + \sigma_w \mathbf{w}
\end{equation}
with $\sigma_w$ such that
\begin{equation}
	\frac{1}{\sigma_w^2} = 2\frac{E_b}{N_0}R\frac{log_2M}{E_s} = \frac{2}{3} \frac{E_b}{N_0}
\end{equation}
since $M=2$ and $E_s = 1$.


\begin{figure}[t]
\centering
\setlength\fheight{0.4\textwidth}
\setlength\fwidth{0.6\textwidth}
\input{./figures/BER_theory.tex}
\caption{BER from expressions~\eqref{eq:BER_bound} and~\eqref{eq:BER_approx}}
\label{fig:BER_theory}
\end{figure}

In this way for all the $E_b/N_0$ values encoding and noise generation are carried out only once. For each $E_b/N_0$ the $\mathbf{r}_{E_b/N_0}$ is decoded to retrieve $\mathbf{\hat{u}}$ and the number of decoded packets $N_{pck, E_b/N_0}$ for that SNR is increased by one. The number of errors is added to the total number of error for that SNR $N_{err, E_b/N_0}$. If the number of error exceeds a certain threshold $N_{min}$ then in the following iterations the decoding for this particular SNR is not carried out in order to speed up the simulation. Moreover an option to use the all zero input $\mathbf{u} = \mathbf{0}$ is introduced, so that the encoding operation is not needed since the encoded vector is $\mathbf{y} = -\mathbf{1}$. 

The BER is then computed as
\begin{equation}
	P_{bit, \frac{E_b}{N_0}} =\frac{N_{err, \frac{E_b}{N_0}}}{N_{pck, \frac{E_b}{N_0}}(\mu+\nu)}
\end{equation}

Let's draw some considerations as first on the BER performance of the code, then on the time required to complete a simulation with different settings.

In Fig.~\ref{fig:BER_1} there is the comparison between the simulated results and the theoretical bounds. The length of the packets in this simulation is $\mu + \nu = 10^4 + 2$ and the number of iterations is $10^5$, with a threshold on the number of errors $N_{min} = 10^3$. The expected precision is in the order of $10^{-8}$. Both a random input and the all zero input word are simulated and it can be seen that the simulated performances do not depend on the randomness of the input. Therefore it is not restrictive to use $\mathbf{u} = \mathbf{0}$ for the soft decoding case.

Notice that for low $E_b/N_0$ the bound~\eqref{eq:BER_bound} is quite loose, while as the SNR increases both the approximated expression~\eqref{eq:BER_approx} and the bound~\eqref{eq:BER_bound} are tight. 

In Fig.~\ref{fig:BER_2} there is the comparison between a Viterbi that backtracks on the complete trellis and a windowed version for 3 different window size $\upsilon$. It can be seen that for $\upsilon = 10 = 5\nu$ the performances in terms of decoding errors are equivalent to the ones of the standard Viterbi, while for smaller window sizes the gap in performances increases up to more than 2 dB for $\upsilon = 3$. 

In Fig.~\ref{fig:BER_3} a comparison between the standard soft decoding approach, where the cost of each transition is proportional to both the sign and the module of the received value, and the less powerful hard decoding approach, where only the sign of the received symbol is considered. 
It can be seen that the soft decoding approach gains about 2.5 dB with respect to the hard decoding one. A particular behavior is exhibited by the hard decoding performances when the all 0 codeword is sent, with a BER nearly 2 orders of magnitude smaller for the same $E_b/N_0$ ratio. 

In Fig.~\ref{fig:BER_577} there is a comparison with the $(5, 7, 7)$ code which is the most performing convolutional code with $\nu = 2$ and $R=1/3$ according to \cite{proakis}. The simulation of the $(5,7,7)$ uses packets of length $\mu + \nu = 10^4 + 2$ and $5\cdot10^{5}$ iterations in order to get an accurate result at 8 dB (50 errors are recorded). It can be seen that for an SNR $E_b/N_0$ lower than 3 dB the $(5,1,7)$ performs better, but after 6 dB it exhibits a gap of about 0.6 dB with $(5,7,7)$.

\begin{figure}[h!]
\centering
\setlength\fheight{0.4\textwidth}
\setlength\fwidth{0.6\textwidth}
\input{./figures/sim_time.tex}
\caption{Simulation time required to complete 100 simulations with different setups}
\label{fig:perf}
\end{figure}

As far as complexity is considered, in Fig.~\ref{fig:perf} there is a comparison between mean execution time for different algorithms and simulation choices. 
For this analysis $N_{min} = \infty$ so that the same number of decoding operations is carried out for the 12 $E_b/N_0 \in [-3, 8]$ values considered. 
In the $y$ axis there is the average time in seconds to complete 100 simulations with each different setting. 
Notice that the difference between the simulation with encoding (with MEX functions) and with the all zero input word is minimal. Instead, if compared with the latter, the windowed versions are all slower since they need to backtrack $\upsilon$ positions in the trellis in order to decode each symbol, i.e. they backtrack approximately $(\mu + \nu)\upsilon$ positions while the standard Viterbi backtracks only from the last state to the first, so $\mu+\nu$ positions. In terms of memory consumption, instead, the windowed version does not need to store the whole trellis to perform the complete backtracking but only the last $\upsilon$ positions. 

\begin{figure*}[h]
\centering
\subfloat[BER from expressions~\eqref{eq:BER_bound} and~\eqref{eq:BER_approx} vs simulated BER]{\setlength\fheight{0.45\textwidth}
\setlength\fwidth{0.4\textwidth}
\input{./figures/BER_1.tex}
\label{fig:BER_1}
}
\hfil
\subfloat[BER for standard Viterbi vs windowed Viterbi for different window sizes]{\setlength\fheight{0.45\textwidth}
\setlength\fwidth{0.4\textwidth}
\input{./figures/BER_2.tex}
\label{fig:BER_2}
}
\hfil
\subfloat[BER for Viterbi with soft decoding vs hard decoding]{\setlength\fheight{0.45\textwidth}
\setlength\fwidth{0.4\textwidth}
\input{./figures/BER_3.tex}
\label{fig:BER_3}
}
\hfil
\subfloat[BER for (5,1,7) vs (5,7,7), simulation, random input]{\setlength\fheight{0.45\textwidth}
\setlength\fwidth{0.4\textwidth}
\input{./figures/BER_577.tex}
\label{fig:BER_577}
}
\end{figure*}


\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% PROBLEM 5 %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Exercise 5}
In this exercise I will derive the generating matrix $\mathbf{G}$ for a binary code with parity check matrix
\begin{equation}
	\mathbf{H} = 
	\begin{bmatrix}
		1&0&1&1&1&0&0&0&1&1\\
		0&1&0&1&1&1&1&0&0&1\\
		0&1&0&0&1&1&1&1&0&1\\
		1&1&1&1&0&0&0&1&1&0\\
		1&0&1&0&0&1&1&1&1&0\\
	\end{bmatrix}
\end{equation}
Let the permutation matrix $\mathbf{\Pi}$ be
\begin{equation}
	\mathbf{\Pi} = 
	\begin{bmatrix}
		1&0&0&0&0&0&0&0&0&0\\
		0&1&0&0&0&0&0&0&0&0\\
		0&0&1&0&0&0&0&0&0&0\\
		0&0&0&0&1&0&0&0&0&0\\
		0&0&0&0&0&1&0&0&0&0\\
		0&0&0&1&0&0&0&0&0&0\\
		0&0&0&0&0&0&1&0&0&0\\
		0&0&0&0&0&0&0&1&0&0\\
		0&0&0&0&0&0&0&0&1&0\\
		0&0&0&0&0&0&0&0&0&1\\
	\end{bmatrix}
\end{equation}
Then $\mathbf{H}$ is the form $\mathbf{H} = [\mathbf{B}, \mathbf{C}]\mathbf{\Pi}$ with $\mathbf{C}$ invertible. The matrix $\mathbf{\hat{H}} = [\mathbf{B}, \mathbf{C}]$ can be obtained by right multiplying the inverse of $\mathbf{\Pi}$ to $\mathbf{H}$. Notice that $\mathbf{\Pi}$ is orthogonal and $\mathbf{\Pi}^T = \mathbf{\Pi}^{-1}$. Then 
\begin{equation}
\mathbf{\hat{H}} =
	\begin{bmatrix}
		1&0&1&1&0&1&0&0&1&1\\
		0&1&0&1&1&1&1&0&0&1\\
		0&1&0&1&1&0&1&1&0&1\\
		1&1&1&0&0&1&0&1&1&0\\
		1&0&1&0&1&0&1&1&1&0\\
	\end{bmatrix}
\end{equation}
Let's apply Gauss-Jordan elimination to $[\mathbf{I}_5, \mathbf{\hat{H}}]$, in particular the following steps:
\begin{equation}
	\left[
	\begin{array}{ccccccccccccccc}
		1&0&0&0&0&1&0&1&1&0&1&0&0&1&1\\
		1&1&0&0&0&1&1&1&0&1&0&1&0&1&0\\
		1&1&1&0&0&1&0&1&1&0&0&0&1&1&1\\
		1&0&0&1&0&0&1&0&1&0&0&0&1&0&1\\
		0&0&0&0&1&1&0&1&0&1&0&1&1&1&0\\
	\end{array}
	\right]
	\begin{array}{l}
		1 \\
		1 + 2 \\
		1 + 2 + 3 \\
		1 + 4 \\
		5 \\
	\end{array}
\end{equation}
\begin{equation}
	\left[
	\begin{array}{ccccccccccccccc}
		1&0&0&0&0&1&0&1&1&0&1&0&0&1&1\\
		1&1&0&0&0&1&1&1&0&1&0&1&0&1&0\\
		1&1&1&0&0&1&0&1&1&0&0&0&1&1&1\\
		0&1&1&1&0&1&1&1&0&0&0&0&0&1&0\\
		0&1&0&1&1&0&0&0&1&0&0&0&0&0&1\\
	\end{array}
	\right]
	\begin{array}{l}
		1 \\
		2 \\
		3 \\
		3 + 4 \\
		2 + 4 + 5 \\
	\end{array}
\end{equation}
The final result is in the form $[\mathbf{C}^{-1}, \, \mathbf{C}^{-1}\mathbf{B}, \, \mathbf{I}_{5}]$ with $\mathbf{A} = \mathbf{C}^{-1}\mathbf{B}$
\begin{equation}
	[\mathbf{C}^{-1}, \mathbf{C}^{-1}\mathbf{B}, \mathbf{I}_{5}] = 
	\left[
	\begin{array}{ccccccccccccccc}
		1&0&1&0&1&0&1&0&0&0&1&0&0&0&0\\
		1&0&1&1&0&0&0&0&0&1&0&1&0&0&0\\
		1&1&0&0&1&0&1&0&0&0&0&0&1&0&0\\
		0&1&1&1&0&1&1&1&0&0&0&0&0&1&0\\
		0&1&0&1&1&0&0&0&1&0&0&0&0&0&1\\
	\end{array}
	\right]
	\begin{array}{l}
		1 + 4 + 5 \\
		2 + 4 \\
		3 + 4 + 5 \\
		4 \\
		5 \\
	\end{array}
\end{equation}
so that 
\begin{equation}
	\mathbf{A} = 
	\begin{bmatrix}
		0&1&0&0&0\\0&0&0&0&1\\0&1&0&0&0\\1&1&1&0&0\\0&0&0&1&0\\
	\end{bmatrix}
\end{equation}
and 
\begin{equation}
	\mathbf{\tilde{H}} = 
	\begin{bmatrix}

		0&1&0&0&0&1&0&0&0&0\\0&0&0&0&1&0&1&0&0&0\\0&1&0&0&0&0&0&1&0&0\\1&1&1&0&0&0&0&0&1&0\\0&0&0&1&0&0&0&0&0&1\\
	\end{bmatrix}
\end{equation}
Then it is possible to define the generating matrix $\mathbf{\tilde{G}}$ associated to $\mathbf{\tilde{H}}$:
\begin{equation}
	\mathbf{\tilde{G}} = 
	\begin{bmatrix}
		\mathbf{I}_k \\
		- \mathbf{A} \\
	\end{bmatrix}
\end{equation}
where $-\mathbf{A} = \mathbf{A}$ because this is a code in Galois Field $\mathbb{F}_2$. It can be seen that $\mathbf{\tilde{H}}\mathbf{\tilde{G}} = \mathbf{0}$.

By pre-multiplying the transpose of the permutation matrix $\mathbf{\Pi}^T$ to $\mathbf{\tilde{G}}$ it is possible to get the generating matrix $\mathbf{G}$ that corresponds to $\mathbf{H}$:
\begin{equation}
	\mathbf{G} = 
	\mathbf{\Pi}^T
	\begin{bmatrix}
		\mathbf{I}_k \\
		\mathbf{A} \\
	\end{bmatrix} = 
	\begin{bmatrix}
		1&0&0&0&0\\
		0&1&0&0&0\\
		0&0&1&0&0\\
		0&1&0&0&0\\
		0&0&0&1&0\\
		0&0&0&0&1\\
		0&0&0&0&1\\
		0&1&0&0&0\\
		1&1&1&0&0\\
		0&0&0&1&0\\
	\end{bmatrix}
\end{equation}
Note that $\mathbf{H}\mathbf{G} = \mathbf{0}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% PROBLEM 7 %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Exercise 7}

\begin{figure}[t]
\centering
\begin{tikzpicture}[auto, thick, node distance=3cm, ->,>=latex',shorten >=1pt]

	\node [trellisnode] (00) {$0$};
	\node [below=0.1cm of 00] (n3) {$l=0$};
	\node [trellisnode, right of=00] (01) {$\infty$};
	\node [below=0.1cm of 01] (n4) {$l=1$};

	\node [trellisnode, right of=01] (02) {$\infty$};
	\node [below=0.1cm of 02] (n4) {$l=2$};
	\node [trellisnode, above=1cm of 02] (12) {$7$};
	\node [trellisnode, above=1cm of 12] (22) {$\infty$};
	\node [trellisnode, above=1cm of 22] (32) {$5$};
	\node [trellisnode, left of=22] (21) {$4$};

	\node [trellisnode, right of=02] (03) {$11$};
	\node [below=0.1cm of 03] (n5) {$l=3$};
	\node [trellisnode, above=1cm of 03] (13) {$6$};
	\node [trellisnode, above=1cm of 13] (23) {$7$};
	\node [trellisnode, above=1cm of 23] (33) {$8$};

	\node [trellisnode, right of=03] (04) {$10$};	
	\node [below=0.1cm of 04] (n6) {$l=4$};
	\node [trellisnode, above=1cm of 04] (14) {$9$};
	\node [trellisnode, above=1cm of 14] (24) {$6$};
	\node [trellisnode, above=1cm of 24] (34) {$8$};

	\node [trellisnode, right of=04] (05) {$13$};
	\node [below=0.1cm of 05] (n7) {$l=5$};
	\node [trellisnode, above=1cm of 05] (15) {$9$};
	\node [trellisnode, above=1cm of 15] (25) {$9$};
	\node [trellisnode, above=1cm of 25] (35) {$7$};
	\node [,above left=0.01cm of 15] (n1) {$3$};
	\node [,above right=0.01cm of 14] (n2) {$0$};


	\path[every node]
	(00) edge node[above] {$\infty$} (01)
		edge[red] node[above] {$4$} (21)
	(01) edge node[above] {$\infty$} (02)
	 	edge node[above] {$4$} (22)	
 	(21) edge node[above] {$3$} (12)
 		edge[red] node[above] {$1$} (32)
	(12) edge node[above] {$4$} (03)
		edge node[above] {$0$} (23)
	(32) edge[red] node[above] {$1$} (13)
		edge node[above] {$3$} (33)
	(13) edge[red] node[above] {$4$} (04)
		edge node[above] {$0$} (24)
	(23) edge node[above] {$1$} (34)
	(33) edge node[above] {$1$} (14)
	(14) edge node[above] {$4$} (05)
		edge (25)
	(24) edge (15)
		edge node[above] {$1$} (35);
	

\end{tikzpicture}
\caption{Trellis for $l \in [0, 5]$ for a $(5,7,7,7)$ binary convolutional code. On the edge there is the cost of each transition $\mathcal{C}(\mathbf{b}, u)$, inside the node the aggregate cost $\Gamma_l(\mathbf{a})$}
\label{fig:trellis_min}
\end{figure}

In this exercise I will describe a Viterbi-like algorithm to find the minimum Hamming weight $d_{min}$ of the code. The idea is to use the Hamming weight as cost of each transition, so that if $(\mathbf{b}, u)$ is a neighbor of $\mathbf{a}$ then
\begin{equation}
	\mathcal{C}(\mathbf{b}, u) = \|\outf{\mathbf{b}, u}\|_H
\end{equation}
Moreover define $\Gamma_l(\mathbf{a})$ as aggregate cost of the path that exits from $\mathbf{0}$ and in $l$ steps reaches state $\mathbf{a}$.

Then the algorithms has to identify $d_{min}$ as the minimum aggregate cost among the paths that start from state $\mathbf{0}$ and go back to it, since we are dealing with a terminated code. If there is a path that starts from $\mathbf{0}$, goes back to this state and then exits again then it will surely have an higher cost than the one that continues to visit $\mathbf{0}$ from that point on. For this purpose we can create a trellis in a Viterbi-like style, by however making infeasible to choose the transitions from $\mathbf{0}$ to itself, with associated output of Hamming weight equal to zero. In particular let $\mathcal{C}(\mathbf{0}, 0) = \infty$.

%Indeed, by choosing this transition at the first step of the trellis, it is as like postponing the search of the codeword of minimum weight at the second step. Therefore at the first step the algorithm exits from state $\mathbf{0}$, then at some time will return to it from a state $\mathbf{b} \ne \mathbf{0}$. If a this point the transition $(\mathbf{0}, 0)->\mathbf{0}$ is allowed, it will propagate the cost of the path with less hops to get back to $\mathbf{0}$ until a path of lower cost will be reached. This means that all the costs of paths with higher cost and higher number of hops will not be considered.

The algorithm has an initialization phase where 
\begin{equation}
\begin{cases}
	\Gamma_0(\mathbf{0}) = 0 \\
	\Gamma_0(\mathbf{b}) = \infty & \mathbf{b} \ne \mathbf{0} \\

\end{cases}
\end{equation}	
Then, for $l \in [1, \mu + \nu]$ with $\mu$ the length of the considered input and $\nu$ the memory of the code, and for each state $\mathbf{a}$, consider the neighbors set $\mathcal{N}(\mathbf{a})$ to find the transition of minimum cost:
\begin{equation}
	(\mathbf{\hat{b}}, \hat{u}) = \argmin_{(\mathbf{b}, u) \in \mathcal{N}(a)} \Gamma_{l-1}(\mathbf{b}) + \mathcal{C}(\mathbf{b}, u)
\end{equation}
Then update 
\begin{equation}
	\Gamma_{l}(\mathbf{a}) = \Gamma_{l-1}(\mathbf{\hat{b}}) + \mathcal{C}(\mathbf{\hat{b}}, \hat{u})
\end{equation}
Eventually choose 
\begin{equation}
	d_{min} = \min_{l \in [1, \mu + \nu]} \Gamma_l(\mathbf{0})
\end{equation}
This algorithm has been implemented in MATLAB and returns the results in Table~\ref{table:dmin} which are consistent with what was found in Exercise 1 and in \cite{proakis}. Notice that convergence to $d_{min}$ is reached in few steps, and typically is not needed to iterate up to $\mu + \nu$. However, in the worst case, there are not visits to state $\mathbf{0}$ unless at the first and last step. In Fig.~\ref{fig:trellis_min} there is the trellis for $l\in[0,5]$ that is built by the algorithm for a $(5,7,7,7)$ binary convolutional code. The path in red is the one that yields a codeword with weight $d_{min}$, and it can be seen that it is not the one with the smallest number of hops to go back to $\mathbf{0}$.

\begin{table}[h]
	\centering
	\begin{tabular}{c|c}
		\toprule
		Code (octal) & $d_{min}$ \\
		\midrule
		(5,7) & 5 \\
		(5,1,7) & 6 \\
		(5,7,7) & 8 \\
		(5,7,7,7) & 10 \\
	\end{tabular}
	\caption{$d_{min}$ of different binary convolutional codes found by the algorithm}
	\label{table:dmin}
\end{table}

\begin{thebibliography}{10}

\bibitem{erseghe} Tomaso Erseghe, Channel Coding, \emph{A graduate course blueprint}, 2015

\bibitem{proakis} John G. Proakis, Masoud Salehi, Digital communications, 5th ed., McGraw-Hill, 2008

\bibitem{moler} Cleve Moler, Numerical Computing with MATLAB, SIAM, 2004

\end{thebibliography}

\end{document}
